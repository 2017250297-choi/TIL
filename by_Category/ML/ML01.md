# ML
*test입니다.
## 먼저 알고리즘에 대하여

문제를 해결하기위해 정해진 일련의 절차를 알고리즘이라고 한다.
단순한 알고리즘을 통해 다양한 변인들이 있는 복잡한 상황에서의 예측, 분류 등의 문제를 해결하기는 어렵다.
그러나 통계적, 수학적 모델들을 이용한 알고리즘을 사용하면 가능하다.

## 문제의 유형: 회귀 vs 분류

- 회귀: 입력값에 대한 연속적(float)인 다른 값의 예측
- 분류: 비 연속적인 결과를 예측해야할 때 (pass/fail - binary, LG/삼성/애플 - multiclass, ...)

둘중 어느것을 사용할지는 사용자가 재량껏 선택해야한다.

## 학습의 방법 : 지도 vs 비지도 vs 강화학습
- 지도학습 - 회귀/분류. 라벨링이 되어있는 데이터를 사용한다. 즉 입력값에 대해 사람이 원하는 출력이 있다. 학습의 정확도를 쉽게 파악할 수 있다는 장점이 있지만 라벨링(또는 annotation)된 대량의 데이터가 없으면 노가다 없인 활용이 불가능하다.
- 비지도 학습 - 클러스터링, 시각화, 차원축소, 연관규칙학습 등. 라벨이 없는 데이터를 사용한다. annoatation 없이도 학습이 가능하여 데이터로부터 인간이 찾지못한 통찰을 얻을 수 있다. 학습의 결과가 올바른지 확인하기 어렵다.
- 강화학습 - 분류 할 수 있는 데이터가 존재하지 않거나, 존재하더라도 정답이 정해져 있지 않으며 **행동에 대한 지연된 보상을 받으며 학습**하는 것이다. 모든 향동의 조합에 대해 지도/비지도 학습을 시키는 것은 현실적으로 불가능하지만 강화학습을 사용하면 된다.
  단, 행동(Action)을 위한 행동 목록은 사전에 정의가 되어야 한다.

## 선형회귀

입력과 출력간의 '선형적 관계'가 있고 그것을 통해 예측이 가능하다고 가정한다. 입력변수의 수가 2개 이상이면 다중선형회귀 라고 불린다. 이를 통해 가설 H(x)를 다음과 같이 수립한다.

이 가설이 실제 관측값과의 차이를 측정하기 위해 손실함수(cost function, loss function)를 정의해 주어야하는데, 선형회귀에서는 보통 Mean Squared Error를 사용한다.

loss function의 값에 따라 W와 b의 값이 조금씩 변하며 loss가 최소가 되는 지점을 찾게되는데 이를 위해 optimizer들을 사용하고, 가장 많이 쓰이는 것은 '경사하강법(Gradient Descent Method)'이다.

## 경사하강법

loss function의 경사를 따라 조금씩 이동하는 방식인데, 이때 한번에 이동하는 거리를 learning rate라고 한다. 너무 작으면 global minimum을 찾기 어렵고, 너무 크면 overshooting에 의해 수렴하지 않을 수 있다.

## 데이터 셋 분할

주어진 데이터를 라벨이 주어진 학습에 사용할 부분과 라벨이 없는 테스팅에 사용할 부분으로 나눌 수 있다. 트레이닝 셋에서 다시 validation set을 빼내는데, 이것은 모델의 성능을 검증하고 optimizer를 바꾸는 등의 튜닝하기 위해 쓰인다(모의고사). testing set은 라벨 없이 최종적으로 형성된 모델의 실제 사용에서의 성능을 검증하기 위한 것이다(수능).
